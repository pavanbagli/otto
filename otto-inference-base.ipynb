{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q tensorflow-recommenders\n!pip install -q --upgrade tensorflow-datasets\n!pip install -q scann","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\nimport zipfile\nimport json\nimport tensorflow as tf\nimport tensorflow_recommenders as tfrs\nimport csv\nfrom tqdm import tqdm\nfrom typing import Dict, Text","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:09:51.531649Z","iopub.execute_input":"2022-12-30T09:09:51.532976Z","iopub.status.idle":"2022-12-30T09:09:55.082726Z","shell.execute_reply.started":"2022-12-30T09:09:51.532880Z","shell.execute_reply":"2022-12-30T09:09:55.081688Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2022-12-30 09:09:51.547612: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-12-30 09:09:52.574716: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n2022-12-30 09:09:52.574933: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n2022-12-30 09:09:52.574947: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","output_type":"stream"}]},{"cell_type":"code","source":"print('TensorFlow version: {}'.format(tf.__version__))\nprint('TensorFlow Recommender version: {}'.format(tfrs.__version__))\nprint('TensorFlow Raking')\nprint('TensorFlow ScaNN')","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:09:56.729101Z","iopub.execute_input":"2022-12-30T09:09:56.729704Z","iopub.status.idle":"2022-12-30T09:09:56.737216Z","shell.execute_reply.started":"2022-12-30T09:09:56.729667Z","shell.execute_reply":"2022-12-30T09:09:56.735281Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"TensorFlow version: 2.11.0\nTensorFlow Recommender version: v0.7.2\nTensorFlow Raking\nTensorFlow ScaNN\n","output_type":"stream"}]},{"cell_type":"code","source":"TEST_FILE = '/kaggle/input/otto-recommender-system/test.jsonl'\nSCANN_DIR = '/kaggle/input/otto-basemodel-v1/kaggle/working/serving_model/ScaNN_Model'\nMODEL_DIR = '/kaggle/input/otto-basemodel-v1/kaggle/working/serving_model/Norm_Model'\nSUBMIT_FILE = '/kaggle/working/submission_file4.csv'\nOUTPUT_DIR = '/kaggle/working/'","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:09:58.464689Z","iopub.execute_input":"2022-12-30T09:09:58.465771Z","iopub.status.idle":"2022-12-30T09:09:58.471280Z","shell.execute_reply.started":"2022-12-30T09:09:58.465720Z","shell.execute_reply":"2022-12-30T09:09:58.470354Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# **Helper Functions**","metadata":{}},{"cell_type":"code","source":"def preprocess(df):\n    initial_data, features={}, {}\n    session_list, item_A_list, ts_list = [], [], []\n   \n    for index in df.index:\n        session_id = str(df['session'][index])\n        session_list.append(session_id)\n        item_A_list.append(str(df['events'][index][0]['type']))\n        ts_list.append(df['events'][index][0]['ts'])\n        click_list, cart_list, order_list = [], [], []\n    \n        for event in df['events'][index]:\n            if event['type']=='clicks':\n                click_list.append(event['aid'])\n            elif event['type']=='carts':\n                cart_list.append(event['aid'])\n            else:\n                order_list.append(event['aid'])\n        initial_data[session_id] = {'clicks':click_list, 'carts':cart_list, 'orders':order_list}\n    \n    features={'session_list':session_list, 'item_A_list':item_A_list, 'ts_list':ts_list}\n    return features, initial_data\n\ndef get_rating(item_A, item_B_list, ts):\n    ratings = {}\n    for item_B in item_B_list:\n        A, B, ratings[item_B] = rat_model({\n        \"item_A\": np.array([item_A]),\n        \"item_B\": np.array([item_B]),\n        \"ts\":np.array([ts])\n            })\n    return {k: v for k, v in sorted(ratings.items(), key=lambda item: item[1],reverse=True)}\n        \ndef postprocess(i_aid, aid_rating):\n    predict_click_list=[]\n    predict_cart_list=[]\n    predict_order_list=[]\n    final_aid={}\n    \n    filter_aid = dict(filter(lambda elem: elem[1] > 1.5,aid_rating.items()))\n    for key, value in filter_aid.items():\n        if value.numpy() > 2.999:\n            predict_click_list.append(key.decode())\n            predict_cart_list.append(key.decode())\n            predict_order_list.append(key.decode())\n        \n        elif 2.2 <= value.numpy() <= 2.999:\n            predict_click_list.append(key.decode())\n            predict_cart_list.append(key.decode())\n        \n        elif value.numpy() >= 1.2:\n            predict_click_list.append(key.decode())\n        \n        else:\n            pass\n    \n    final_click_list = i_aid['clicks'] + predict_click_list\n    final_cart_list = i_aid['carts'] + predict_cart_list\n    final_order_list = i_aid['orders'] + predict_order_list\n    \n    final_aid['clicks'] = final_click_list\n    final_aid['carts'] = final_cart_list\n    final_aid['orders'] = final_order_list\n        \n    return final_aid\n\ndef writefile(record, header):\n    with open(SUBMIT_FILE, 'a') as file:\n        writer = csv.writer(file)\n        if header:\n            writer.writerow(record)\n        else:\n            writer.writerows(record)\n    return","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:10:01.094531Z","iopub.execute_input":"2022-12-30T09:10:01.094924Z","iopub.status.idle":"2022-12-30T09:10:01.113003Z","shell.execute_reply.started":"2022-12-30T09:10:01.094889Z","shell.execute_reply":"2022-12-30T09:10:01.111798Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"ret_model = tf.saved_model.load(SCANN_DIR)\nrat_model = tf.keras.models.load_model(MODEL_DIR)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:10:03.543004Z","iopub.execute_input":"2022-12-30T09:10:03.543510Z","iopub.status.idle":"2022-12-30T09:10:30.510909Z","shell.execute_reply.started":"2022-12-30T09:10:03.543455Z","shell.execute_reply":"2022-12-30T09:10:30.509867Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2022-12-30 09:10:06.652112: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 237517312 exceeds 10% of free system memory.\n2022-12-30 09:10:23.233361: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 237517312 exceeds 10% of free system memory.\n2022-12-30 09:10:23.233412: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 237517312 exceeds 10% of free system memory.\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nchunk_size=100000\nnum_lines = sum(1 for line in open(TEST_FILE))\nnum_chunks= int(np.ceil(num_lines / chunk_size))\n\nprint('Number of lines: ',num_lines)\nprint('Number of chunks: ',num_chunks)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:10:33.985159Z","iopub.execute_input":"2022-12-30T09:10:33.985861Z","iopub.status.idle":"2022-12-30T09:10:38.683060Z","shell.execute_reply.started":"2022-12-30T09:10:33.985812Z","shell.execute_reply":"2022-12-30T09:10:38.682060Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Number of lines:  1671803\nNumber of chunks:  17\nCPU times: user 525 ms, sys: 199 ms, total: 723 ms\nWall time: 4.69 s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nchunks = pd.read_json(TEST_FILE, lines=True, chunksize=chunk_size)\n\nheader_record=['session_type, labels']\nheader_flag=True\nwritefile(header_record, header_flag)","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:10:41.769076Z","iopub.execute_input":"2022-12-30T09:10:41.769472Z","iopub.status.idle":"2022-12-30T09:10:41.777757Z","shell.execute_reply.started":"2022-12-30T09:10:41.769438Z","shell.execute_reply":"2022-12-30T09:10:41.776629Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"CPU times: user 1.88 ms, sys: 1.16 ms, total: 3.04 ms\nWall time: 3.14 ms\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nheader_flag=False\nfor chunk_num, chunk in enumerate(chunks):\n    submit_records = []\n    rating={}\n    print('Processing chunk # : ', chunk_num)\n    \n    features, initial_data = preprocess(chunk)\n    _, retrieved_data = ret_model(tf.constant(features['item_A_list']))\n    \n#     for i in range(len(features['item_A_list'])):\n#         item_A = [features['item_A_list'][i]] * len(retrieved_data[i]) \n#         item_B = retrieved_data[i]\n#         ts = [features['ts_list'][i]] * len(retrieved_data[i])\n        \n#         embed_A, embed_B, rating[i] =  rat_model({\n#             \"item_A\": np.array(item_A),\n#             \"item_B\": np.array(item_B),\n#             \"ts\":np.array(ts)\n#             })\n        \n    print(len(retrieved_data))\n#     break\n    \n    \n    \n#     for index in chunk.index:\n#         events = chunk['events'][index]\n#         session_id = str(chunk['session'][index])\n#         first_aid =  str(events[0]['aid'])\n#         session_start_ts = events[0]['ts']\n#         print('first_aid', first_aid)\n                                \n#         i_aid = preprocess(events)\n#         _, p_aid_list = ret_model(tf.constant([first_aid]))\n#         aid_rating = get_rating(first_aid, p_aid_list[0].numpy(), session_start_ts)\n#         f_aid = postprocess(i_aid, aid_rating)\n        \n#         for key, value in f_aid.items():\n#             temp_list=[]\n#             if len(value) > 0:\n#                 session_type = session_id + '_' + str(key)\n#                 f_aid_list = \" \".join(value)\n#                 temp_list.append(session_type)\n#                 temp_list.append(f_aid_list)\n#                 submit_records.append(temp_list)    \n        \n#         print(submit_records)\n#         break;\n# #     writefile(submit_records, header_flag)\n# #     break","metadata":{"execution":{"iopub.status.busy":"2022-12-30T09:10:46.588019Z","iopub.execute_input":"2022-12-30T09:10:46.588401Z","iopub.status.idle":"2022-12-30T09:13:24.289364Z","shell.execute_reply.started":"2022-12-30T09:10:46.588363Z","shell.execute_reply":"2022-12-30T09:13:24.288243Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Processing chunk # :  0\n100000\nProcessing chunk # :  1\n100000\nProcessing chunk # :  2\n100000\nProcessing chunk # :  3\n100000\nProcessing chunk # :  4\n100000\nProcessing chunk # :  5\n100000\nProcessing chunk # :  6\n100000\nProcessing chunk # :  7\n100000\nProcessing chunk # :  8\n100000\nProcessing chunk # :  9\n100000\nProcessing chunk # :  10\n100000\nProcessing chunk # :  11\n100000\nProcessing chunk # :  12\n100000\nProcessing chunk # :  13\n100000\nProcessing chunk # :  14\n100000\nProcessing chunk # :  15\n100000\nProcessing chunk # :  16\n71803\nCPU times: user 3min 47s, sys: 780 ms, total: 3min 48s\nWall time: 2min 37s\n","output_type":"stream"}]},{"cell_type":"code","source":"_, p_aid_list = ret_model(tf.constant(['59625','59625','99163']))\nprint(p_aid_list)        \n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"A, B, ratings = rat_model({\n        \"item_A\": np.array(['59625','59625','59625','59625']),\n        \"item_B\": np.array(['1702672','725615','1702672','725615']),\n        \"ts\":np.array([1234, 1234,1234, 1234])\n            })\n\nprint(ratings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}