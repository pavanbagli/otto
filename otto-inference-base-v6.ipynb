{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q tensorflow-recommenders\n!pip install -q scann","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np \nimport time\nimport pandas as pd \nimport os\nimport glob\nimport json\nimport tensorflow as tf\nimport tensorflow_recommenders as tfrs\nimport csv\nimport gc\nimport itertools\n# import cudf\n# import cupy as cp","metadata":{"execution":{"iopub.status.busy":"2023-01-19T16:00:58.464161Z","iopub.execute_input":"2023-01-19T16:00:58.464972Z","iopub.status.idle":"2023-01-19T16:01:04.355769Z","shell.execute_reply.started":"2023-01-19T16:00:58.464894Z","shell.execute_reply":"2023-01-19T16:01:04.353597Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2023-01-19 16:00:58.492751: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2023-01-19 16:00:58.721139: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n2023-01-19 16:00:58.721175: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n2023-01-19 16:01:00.496729: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n2023-01-19 16:01:00.496946: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n2023-01-19 16:01:00.496963: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","output_type":"stream"}]},{"cell_type":"code","source":"print('TensorFlow version: {}'.format(tf.__version__))\nprint('TensorFlow Recommender version: {}'.format(tfrs.__version__))","metadata":{"execution":{"iopub.status.busy":"2023-01-19T16:01:04.363977Z","iopub.execute_input":"2023-01-19T16:01:04.364443Z","iopub.status.idle":"2023-01-19T16:01:04.376374Z","shell.execute_reply.started":"2023-01-19T16:01:04.364406Z","shell.execute_reply":"2023-01-19T16:01:04.372944Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"TensorFlow version: 2.11.0\nTensorFlow Recommender version: v0.7.2\n","output_type":"stream"}]},{"cell_type":"code","source":"TEST_FILE = '/kaggle/input/otto-recommender-system/test.jsonl'\nITEM_DIR = '/kaggle/input/otto-preprocessed-items-1'\nSCANN_DIR = '/kaggle/input/otto-basemodel-v7/serving_model/ScaNN_Model'\nMODEL_DIR = '/kaggle/input/otto-basemodel-v7/serving_model/Norm_Model'\nSUBMIT_FILE = '/kaggle/working/sample_submission.csv'\nOUTPUT_DIR = '/kaggle/working/'","metadata":{"execution":{"iopub.status.busy":"2023-01-19T16:01:07.738886Z","iopub.execute_input":"2023-01-19T16:01:07.739364Z","iopub.status.idle":"2023-01-19T16:01:07.747770Z","shell.execute_reply.started":"2023-01-19T16:01:07.739326Z","shell.execute_reply":"2023-01-19T16:01:07.746090Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def preprocess(df):\n    print('   Preprocessing...')\n    start = time.perf_counter()\n    \n    session_list, item_A_list, item_A, ts_list = [], [], [], []\n    ip_session, ip_item =[],[]\n    \n    for index in df.index:\n        session_list.append([str(df['session'][index])]*500)\n        item_A_list.append([str(df['events'][index][0]['aid'])]*500)\n        item_A.append(str(df['events'][index][0]['aid']))\n        ts_list.append([df['events'][index][0]['ts']]*500)\n        \n        type_check=[]\n        for event in df['events'][index]:\n            if event['type']=='orders':\n                ip_session.append(str(df['session'][index]) + '_' + 'orders')\n                ip_item.append(str(event['aid']))\n                \n                ip_session.append(str(df['session'][index]) + '_' + 'carts')\n                ip_item.append(str(event['aid']))\n                \n                ip_session.append(str(df['session'][index]) + '_' + 'clicks')\n                ip_item.append(str(event['aid']))\n            elif event['type']=='carts':\n                ip_session.append(str(df['session'][index]) + '_' + 'carts')\n                ip_item.append(str(event['aid']))\n                \n                ip_session.append(str(df['session'][index]) + '_' + 'clicks')\n                ip_item.append(str(event['aid']))\n            else:\n                ip_session.append(str(df['session'][index]) + '_' + 'clicks')\n                ip_item.append(str(event['aid']))\n                \n    input_df=pd.DataFrame({\n        'session_type': ip_session,\n        'labels': ip_item\n        })\n    \n    repeat_df = pd.DataFrame({\n        'session' : list(itertools.chain(*session_list)),\n        'item_A' : list(itertools.chain(*item_A_list)),\n        'ts' : list(itertools.chain(*ts_list))\n        })\n    end = time.perf_counter()\n    print('   ',(end - start)* 10**6)\n    return item_A, input_df, repeat_df\n\ndef retrieve_items(item_A):\n    print('   Retrieval...')\n    start = time.perf_counter()\n    \n    _, retrieved_items = scann_index(tf.constant(item_A))\n    retrieved_items=tf.reshape(retrieved_items,[retrieved_items.shape[0] * retrieved_items.shape[1]])\n    retrieved_items=retrieved_items.numpy()\n    retrieved_items=[x.decode() for x in retrieved_items]\n    end = time.perf_counter()\n    print('   ',(end - start)* 10**6)\n    return retrieved_items\n\ndef rate_items(rep_data_df):\n    print('   Rating...') \n    start = time.perf_counter()\n    \n    _, _, rating_data = rat_model({\n    'item_A': tf.constant(rep_data_df['item_A']),\n    'item_B': tf.constant(rep_data_df['item_B']),\n    'ts': tf.constant(rep_data_df['ts']) \n    })\n    end = time.perf_counter()\n    print('   ',(end - start)* 10**6)\n    return tf.reshape(rating_data, [rating_data.shape[0]])\n\ndef postprocessing(repeat_df):\n    print('   Postprocessing...')\n    start = time.perf_counter()\n    \n    repeat_df = repeat_df.sort_values(by=['session','rating'],ascending=False)\n    repeat_df.to_csv(f'zrepeat_df{chunk_num}.csv', header=False, index=False)\n    \n    top20_df = repeat_df.groupby('session').head(20)\n    click_df = top20_df.copy() \n    cart_df = top20_df.copy()\n    order_df = top20_df.copy()\n       \n    click_df['session_type'] = click_df['session'].astype(str) + '_clicks'\n    click_df = click_df[['session_type', 'labels']]\n    \n    cart_df['session_type'] = cart_df['session'].astype(str) + '_carts'\n    cart_df = cart_df[['session_type', 'labels']]\n    \n    order_df['session_type'] = order_df['session'].astype(str) + '_orders'\n    order_df = order_df[['session_type', 'labels']]\n    \n    pred_df = pd.concat([click_df, cart_df, order_df])\n    \n    end = time.perf_counter()\n    print('   ',(end - start)* 10**6)\n    return pred_df ","metadata":{"execution":{"iopub.status.busy":"2023-01-19T16:19:32.055045Z","iopub.execute_input":"2023-01-19T16:19:32.055798Z","iopub.status.idle":"2023-01-19T16:19:32.087072Z","shell.execute_reply.started":"2023-01-19T16:19:32.055753Z","shell.execute_reply":"2023-01-19T16:19:32.085893Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# ret_model = tf.saved_model.load(SCANN_DIR)\nrat_model = tf.saved_model.load(MODEL_DIR)","metadata":{"execution":{"iopub.status.busy":"2023-01-19T16:01:12.237637Z","iopub.execute_input":"2023-01-19T16:01:12.238745Z","iopub.status.idle":"2023-01-19T16:01:24.269207Z","shell.execute_reply.started":"2023-01-19T16:01:12.238668Z","shell.execute_reply":"2023-01-19T16:01:24.267398Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"2023-01-19 16:01:14.540523: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n2023-01-19 16:01:14.540575: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n2023-01-19 16:01:14.540609: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (bdd472ed3df2): /proc/driver/nvidia/version does not exist\n2023-01-19 16:01:14.540924: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nitems_dataset = tf.data.Dataset.load(ITEM_DIR)\nitems_dataset = items_dataset.batch(batch_size=256,num_parallel_calls=tf.data.AUTOTUNE)\nitemlist_unique = np.unique(np.concatenate(list(items_dataset)))","metadata":{"execution":{"iopub.status.busy":"2023-01-19T16:08:50.637992Z","iopub.execute_input":"2023-01-19T16:08:50.639132Z","iopub.status.idle":"2023-01-19T16:09:25.815245Z","shell.execute_reply.started":"2023-01-19T16:08:50.639050Z","shell.execute_reply":"2023-01-19T16:09:25.813746Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"%%time\nscann_index = tfrs.layers.factorized_top_k.ScaNN(rat_model.item_A_model, k=500)\nscann_index.index_from_dataset(\n  tf.data.Dataset.zip((items_dataset, items_dataset.map(rat_model.item_B_model)))\n)","metadata":{"execution":{"iopub.status.busy":"2023-01-19T16:09:48.059687Z","iopub.execute_input":"2023-01-19T16:09:48.060145Z","iopub.status.idle":"2023-01-19T16:11:26.396476Z","shell.execute_reply.started":"2023-01-19T16:09:48.060111Z","shell.execute_reply":"2023-01-19T16:11:26.394541Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"CPU times: user 1min 35s, sys: 43.4 s, total: 2min 18s\nWall time: 1min 38s\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"<tensorflow_recommenders.layers.factorized_top_k.ScaNN at 0x7f52312ed510>"},"metadata":{}}]},{"cell_type":"code","source":"%%time\nchunk_size=100\nnum_lines = sum(1 for line in open(TEST_FILE))\nnum_chunks= int(np.ceil(num_lines / chunk_size))\n\nprint('Number of lines: ',num_lines)\nprint('Number of chunks: ',num_chunks)\n\nchunks = pd.read_json(TEST_FILE, lines=True, chunksize=chunk_size)","metadata":{"execution":{"iopub.status.busy":"2023-01-19T16:19:31.437351Z","iopub.execute_input":"2023-01-19T16:19:31.437856Z","iopub.status.idle":"2023-01-19T16:19:32.053118Z","shell.execute_reply.started":"2023-01-19T16:19:31.437814Z","shell.execute_reply":"2023-01-19T16:19:32.051628Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Number of lines:  1671803\nNumber of chunks:  16719\nCPU times: user 459 ms, sys: 146 ms, total: 605 ms\nWall time: 608 ms\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\nTotalRecords = 0\nTotalClicks = 0\nTotalCarts = 0\nTotalOrders = 0\nchunk_num = 0\nfor chunk in chunks:\n    chunk_start = time.perf_counter()\n    print('Processing chunk # : ', chunk_num)\n    \n    #Preprocessing\n    item_A, input_df, repeat_df = preprocess(chunk)\n            \n    #Retrieve top 50 items for each session\n    repeat_df['item_B'] = retrieve_items(item_A)\n    \n    #Rate the items: ~1(only clicks) ~2(clicks and carts) ~3(clicks, carts, orders)\n    repeat_df['rating'] = rate_items(repeat_df)\n    \n    #Postprocessing\n    del item_A\n    _ = gc.collect()\n    repeat_df = repeat_df.rename(columns={\"item_B\": \"labels\"})\n    pred_df = postprocessing(repeat_df)\n    sub_df = pd.concat([input_df,pred_df])\n    sub_df = (sub_df.groupby('session_type')['labels']\n               .apply(lambda x: ' '.join(set(x.dropna())))\n               .reset_index())\n       \n    sub_df.to_csv(f'submission_{chunk_num}.csv', header=False, index=False)\n        \n    del input_df, repeat_df, pred_df, sub_df \n    _ = gc.collect()\n    chunk_num = chunk_num + 1\n    chunk_end = time.perf_counter()\n    chunk_time = (chunk_end-chunk_start) * 10**6\n    print('   chunk time:', chunk_time)\n    if chunk_num > 4:\n        break\n\nprint('Total Records :', TotalRecords)","metadata":{"execution":{"iopub.status.busy":"2023-01-19T16:19:32.090234Z","iopub.execute_input":"2023-01-19T16:19:32.090733Z","iopub.status.idle":"2023-01-19T16:19:37.076956Z","shell.execute_reply.started":"2023-01-19T16:19:32.090691Z","shell.execute_reply":"2023-01-19T16:19:37.074934Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Processing chunk # :  0\n   Preprocessing...\n    51018.52799998596\n   Retrieval...\n    49457.98199969431\n   Rating...\n    135250.88699952903\n   Postprocessing...\n    189744.90499931562\n   chunk time: 987393.2250002327\nProcessing chunk # :  1\n   Preprocessing...\n    52247.91299951903\n   Retrieval...\n    48839.998000403284\n   Rating...\n    110244.38999993436\n   Postprocessing...\n    185688.8379998054\n   chunk time: 991088.2599997421\nProcessing chunk # :  2\n   Preprocessing...\n    43188.9149995186\n   Retrieval...\n    50356.10499999166\n   Rating...\n    124155.65199989942\n   Postprocessing...\n    188678.73499948473\n   chunk time: 967572.6579998808\nProcessing chunk # :  3\n   Preprocessing...\n    41384.28300029773\n   Retrieval...\n    49950.24900017597\n   Rating...\n    113247.32199955179\n   Postprocessing...\n    199592.7899997696\n   chunk time: 1004113.3960003208\nProcessing chunk # :  4\n   Preprocessing...\n    37981.492999279\n   Retrieval...\n    45097.04499923828\n   Rating...\n    111558.56900040817\n   Postprocessing...\n    205028.23800052283\n   chunk time: 947209.7809994011\nTotal Records : 0\nCPU times: user 6.14 s, sys: 445 ms, total: 6.59 s\nWall time: 4.96 s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n print('writing submission file...')\ndf = pd.DataFrame()\ndf['session_type'] = ''\ndf['labels'] = ''\ndf.to_csv(SUBMIT_FILE, mode='a', index=False)\n                      \ncsv_files = glob.glob(f'{OUTPUT_DIR}/sub*.csv')\nprint(csv_files)\nfor file in csv_files:\n    df = pd.read_csv(file)\n    df.to_csv(SUBMIT_FILE, mode='a', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n#  print('writing cart file...')\n# df = pd.DataFrame()\n# df['session_type'] = ''\n# df['labels'] = ''\n# df.to_csv(SUBMIT_FILE, mode='a', index=False)\n                      \n# csv_files = glob.glob(f'{OUTPUT_DIR}/sub*.csv')\n# print(csv_files)\n# for file in csv_files:\n#     df = pd.read_csv(file)\n#     df.to_csv(SUBMIT_FILE, mode='a', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from cudf import DataFrame\n# df = DataFrame()\n# df['key'] = ['A', 'A', 'A', 'B', 'B', 'B']\n# df['val1'] = ['A1', 'A2', 'A3', 'B1', 'B2', 'B3']\n# df['val2'] = [1,2,3,4,5,6]\n# # groups = df.groupby('key')['val1'].apply(lambda x: concat(x))\n\n# print(df.groupby('key')['val2'].agg('max'))\n# print(df['val2'].max())\n# for key in groups:\n#     print(key)\n# #     print(value)\n# # # Define a function to apply to each row in a group\n# # def mult(df):\n# #   df['out'] = (lambda x: ' '.join(set(x.dropna())))\n# #   return df\n\n# # result = groups.apply(mult)\n# # print(result)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}